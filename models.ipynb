{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65fbb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "import contractions\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Bidirectional, LSTM, Dense, Concatenate, Dropout, Embedding, GlobalMaxPooling1D\n",
    "from tensorflow.keras.metrics import binary_crossentropy, categorical_crossentropy, Accuracy, Recall, Precision\n",
    "from tensorflow.keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4ccf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_feather('train.feather').sample(1000000).reset_index(drop=True)\n",
    "Y = X.pop('key')\n",
    "\n",
    "X_train, X_test, y_train, y_test = split(X, Y, test_size=0.3)\n",
    "\n",
    "def totensor(df):\n",
    "    return tf.convert_to_tensor(np.array(df.map(lambda x: x.tolist()).values.tolist()))\n",
    "\n",
    "X_train = totensor(X_train.squeeze())\n",
    "X_test = totensor(X_test.squeeze())\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d47989a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_filters=128\n",
    "sent_len = 20\n",
    "\n",
    "weights = api.load('glove-wiki-gigaword-200').vectors\n",
    "\n",
    "inp = Input(shape=(sent_len,))\n",
    "\n",
    "emb = Embedding(input_dim=weights.shape[0],\n",
    "                output_dim=weights.shape[1],\n",
    "                embeddings_initializer=Constant(weights),\n",
    "                trainable=False)\n",
    "\n",
    "conv2 = Conv1D(filters=n_filters,\n",
    "              activation='relu',\n",
    "              padding='same',\n",
    "              kernel_size=2)\n",
    "\n",
    "conv3 = Conv1D(filters=n_filters,\n",
    "              activation='relu',\n",
    "              padding='same',\n",
    "              kernel_size=3)\n",
    "\n",
    "conv5 = Conv1D(filters=n_filters,\n",
    "              activation='relu',\n",
    "              padding='same',\n",
    "              kernel_size=5)\n",
    "\n",
    "model2 = GlobalMaxPooling1D()(conv2(emb(inp)))\n",
    "model3 = GlobalMaxPooling1D()(conv3(emb(inp)))\n",
    "model5 = GlobalMaxPooling1D()(conv5(emb(inp)))\n",
    "\n",
    "conc = Concatenate()([model2, model3, model5])\n",
    "decode = Dropout(0.3)(Dense(256)(conc))\n",
    "out = Dense(1, activation='sigmoid')(decode)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.compile(loss=binary_crossentropy, optimizer='adam', metrics=[Accuracy(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13354e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700000 samples, validate on 300000 samples\n",
      "Epoch 1/10\n",
      "700000/700000 [==============================] - 166s 237us/sample - loss: 0.0091 - accuracy: 0.6251 - recall: 0.9980 - val_loss: 0.0830 - val_accuracy: 0.5469 - val_recall: 0.9930\n",
      "Epoch 2/10\n",
      "700000/700000 [==============================] - 167s 238us/sample - loss: 0.0088 - accuracy: 0.6872 - recall: 0.9982 - val_loss: 0.1183 - val_accuracy: 0.6849 - val_recall: 0.9934\n",
      "Epoch 3/10\n",
      "700000/700000 [==============================] - 166s 238us/sample - loss: 0.0086 - accuracy: 0.7056 - recall: 0.9984 - val_loss: 0.1840 - val_accuracy: 0.8476 - val_recall: 0.9920\n",
      "Epoch 4/10\n",
      "700000/700000 [==============================] - 166s 237us/sample - loss: 0.0082 - accuracy: 0.7429 - recall: 0.9984 - val_loss: 0.1506 - val_accuracy: 0.7825 - val_recall: 0.9902\n",
      "Epoch 5/10\n",
      "700000/700000 [==============================] - 166s 237us/sample - loss: 0.0086 - accuracy: 0.7654 - recall: 0.9985 - val_loss: 0.1250 - val_accuracy: 0.6978 - val_recall: 0.9924\n",
      "Epoch 6/10\n",
      "700000/700000 [==============================] - 166s 237us/sample - loss: 0.0085 - accuracy: 0.7974 - recall: 0.9985 - val_loss: 0.1627 - val_accuracy: 0.7499 - val_recall: 0.9841\n",
      "Epoch 7/10\n",
      "700000/700000 [==============================] - 166s 238us/sample - loss: 0.0089 - accuracy: 0.8226 - recall: 0.9986 - val_loss: 0.1375 - val_accuracy: 0.7503 - val_recall: 0.9901\n",
      "Epoch 8/10\n",
      "700000/700000 [==============================] - 167s 238us/sample - loss: 0.0084 - accuracy: 0.8451 - recall: 0.9987 - val_loss: 0.1663 - val_accuracy: 0.8080 - val_recall: 0.9921\n",
      "Epoch 9/10\n",
      "700000/700000 [==============================] - 167s 238us/sample - loss: 0.0088 - accuracy: 0.8507 - recall: 0.9987 - val_loss: 0.1777 - val_accuracy: 0.8389 - val_recall: 0.9909\n",
      "Epoch 10/10\n",
      "700000/700000 [==============================] - 167s 238us/sample - loss: 0.0092 - accuracy: 0.8687 - recall: 0.9987 - val_loss: 0.1996 - val_accuracy: 0.8468 - val_recall: 0.9901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f64c576d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6977ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000/300000 [==============================] - 24s 80us/sample - loss: 0.1996 - accuracy: 0.8468 - recall: 0.9901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1996402103018937, 0.84675336, 0.9901246]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0dc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcb6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test architecture, unused\n",
    "\n",
    "n_filters=256\n",
    "k_size=5\n",
    "embedding_len=300\n",
    "ngram = 10\n",
    "sent_len = 50\n",
    "lstm_count = 512\n",
    "\n",
    "left_in = Input(shape=(k_size, sent_len, embedding_len))\n",
    "main_in = Input(shape=(1, sent_len, embedding_len))\n",
    "right_in = Input(shape=(k_size, sent_len, embedding_len))\n",
    "\n",
    "left_conv = Conv1D(filters=n_filters,\n",
    "                   activation='tanh',\n",
    "                   padding='same',\n",
    "                   kernel_size=ngram,\n",
    "                   input_shape=(sent_len, embedding_len))\n",
    "main_conv = Conv1D(filters=n_filters,\n",
    "                   activation='tanh',\n",
    "                   padding='same',\n",
    "                   kernel_size=ngram,\n",
    "                   input_shape=(1, embedding_len))\n",
    "right_conv = Conv1D(filters=n_filters,\n",
    "                    activation='tanh',\n",
    "                    padding='same',\n",
    "                    kernel_size=ngram,\n",
    "                    input_shape=(sent_len, embedding_len))\n",
    "\n",
    "context_bilstm1 = Bidirectional(LSTM(lstm_count,\n",
    "                                    input_shape=(k_size, n_filters),\n",
    "                                    recurrent_dropout=0.3,\n",
    "                                    return_sequences=True))\n",
    "context_bilstm2 = Bidirectional(LSTM(lstm_count,\n",
    "                                    input_shape=(k_size, n_filters),\n",
    "                                    recurrent_dropout=0.3,\n",
    "                                    return_sequences=True))\n",
    "\n",
    "left_conv = TimeDistributed(context_conv)(left_in)\n",
    "mid_conv = TimeDistributed(main_conv)(main_in)\n",
    "right_conv = TimeDistributed(context_conv)(right_in)\n",
    "\n",
    "left_pool = TimeDistributed(GlobalMaxPooling1D())(left_conv)\n",
    "mid_pool = TimeDistributed(GlobalMaxPooling1D())(mid_conv)\n",
    "right_pool = TimeDistributed(GlobalMaxPooling1D())(right_conv)\n",
    "\n",
    "left_context = context_bilstm1(context_bilstm2(left_pool))\n",
    "right_context = context_bilstm1(context_bilstm2(right_pool))\n",
    "\n",
    "left_encoded = Dropout(0.3)(AttentionWithContext()(left_context))\n",
    "mid_encoded = Dropout(0.3)(Dense(300)(Flatten()(mid_pool)))\n",
    "right_encoded = Dropout(0.3)(AttentionWithContext()(right_context))\n",
    "\n",
    "encoding = Concatenate()([left_encoded, mid_encoded, right_encoded])\n",
    "decoding = Dropout(0.3)(Dense(512)(encoding))\n",
    "\n",
    "model = Model(inputs=[left_in, main_in, right_in], outputs=decoding)\n",
    "model.compile(loss=binary_crossentropy, optimizer='adadelta', metrics=[Accuracy(), Recall()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
